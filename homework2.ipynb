{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CIS 5200: Machine Learning\n",
        "## Homework 2"
      ],
      "metadata": {
        "id": "lIvXJ8GV9Lq9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a4HTcwb59Eaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "163f17be-264c-4fa0-c157-d3d1843d234e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO, OK] Google Colab.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# For autograder only, do not modify this cell.\n",
        "# True for Google Colab, False for autograder\n",
        "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
        "if NOTEBOOK:\n",
        "    print(\"[INFO, OK] Google Colab.\")\n",
        "else:\n",
        "    print(\"[INFO, OK] Autograder.\")\n",
        "    sys.exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Penngrader setup"
      ],
      "metadata": {
        "id": "e1-lu5IJ9R0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "!pip install penngrader-client"
      ],
      "metadata": {
        "id": "o4K9_rWa9TKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69d678d-14d8-4067-e97a-299bbee8cdb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting penngrader-client\n",
            "  Downloading penngrader_client-0.5.2-py3-none-any.whl (10 kB)\n",
            "Collecting dill (from penngrader-client)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/115.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from penngrader-client) (6.0.1)\n",
            "Installing collected packages: dill, penngrader-client\n",
            "Successfully installed dill-0.3.7 penngrader-client-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.yaml\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "nO2IH6cO9VR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97fa4190-9fd0-4863-bad2-2c12e0a48254"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from penngrader.grader import PennGrader\n",
        "\n",
        "# PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "# TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 61224048 # YOUR PENN-ID GOES HERE AS AN INTEGER #\n",
        "SECRET = STUDENT_ID\n",
        "\n",
        "grader = PennGrader('config.yaml', 'CIS5200_FALL_2023_HW2', STUDENT_ID, SECRET)"
      ],
      "metadata": {
        "id": "qt233qEO9YfK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef62b81-ea54-47eb-880e-582828172352"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PennGrader initialized with Student ID: 61224048\n",
            "\n",
            "Make sure this correct or we will not be able to store your grade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset: Wine Quality Prediction\n",
        "\n",
        "Some research on blind wine tasting has suggested that [people cannot taste the difference between ordinary and pricy wine brands](https://phys.org/news/2011-04-expensive-inexpensive-wines.html). Indeed, even experienced tasters may be as consistent as [random numbers](https://www.seattleweekly.com/food/wine-snob-scandal/).\n",
        "\n",
        "In this problem set, we will train some simple linear models to predict wine quality. We'll be using the data from [this repository](https://archive.ics.uci.edu/ml/datasets/Wine+Quality) for both the classification and regression tasks. The following cells will download and set up the data for you."
      ],
      "metadata": {
        "id": "MipzHX9n9Zzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names"
      ],
      "metadata": {
        "id": "LzHV_XSm9bY4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "red_df = pd.read_csv('winequality-red.csv', delimiter=';')\n",
        "\n",
        "X = torch.from_numpy(red_df.drop(columns=['quality']).to_numpy())\n",
        "y = torch.from_numpy(red_df['quality'].to_numpy())\n",
        "\n",
        "# Split data into train/test splits\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data to have zero mean and standard deviation,\n",
        "# and add bias term\n",
        "mu, sigma = X_train.mean(0), X_train.std(0)\n",
        "X_train, X_test = [ torch.cat([((x-mu)/sigma).float(), torch.ones(x.size(0),1)], dim=1)\n",
        "                    for x in [X_train, X_test]]\n",
        "\n",
        "# Transform labels to {-1,1} for logistic regression\n",
        "y_binary_train, y_binary_test = [ (torch.sign(y - 5.5)).long()\n",
        "                                  for y in [y_train, y_test]]\n",
        "y_regression_train, y_regression_test = [ y.float() for y in [y_train, y_test]]"
      ],
      "metadata": {
        "id": "0nQQ3oPx9ezZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Logistic Regression\n",
        "\n",
        "In this first problem, you will implement a logistic regression classifier to classify good wine (`y=1`) from bad wine (`y=-1`). Your professor has arbitrarily decided that good wine has a score of at least 5.5. The classifier is split into the following components:\n",
        "\n",
        "1. Loss (3pts) & gradient (3pts) - given a batch of examples $X$ and labels $y$ and weights for the logistic regression classifier, compute the batched logistic loss and gradient of the loss with *with respect to the model parameters $w$*. Note that this is slightly different from the gradient in Homework 0, which was with respect to the sample $X$.\n",
        "2. Fit (2pt) - Given a loss function and data, find the weights of an optimal logistic regression model that minimizes the logistic loss\n",
        "3. Predict (3pts) - Given the weights of a logistic regression model and new data, predict the most likely class\n",
        "\n",
        "We provide an generic gradient-based optimizer for you which minimizes the logistic loss function, you can call it with `LogisticOptimizer().optimize(X,y)`. It does not need any parameter adjustment.\n",
        "\n",
        "Hint: The optimizer will minimize the logistic loss. So this value of this loss should be decreasing over iterations."
      ],
      "metadata": {
        "id": "iMi0QqDI9jK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticOptimizer:\n",
        "    @staticmethod\n",
        "    def logistic_loss(X, y, w):\n",
        "        # Given a batch of samples and labels, and the weights of a logistic\n",
        "        # classifier, compute the batched logistic loss.\n",
        "        #\n",
        "        # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n",
        "        #     of dimension d\n",
        "        #\n",
        "        # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n",
        "        #\n",
        "        # w := Tensor(float) of size(d,) --- This is the weights of a logistic\n",
        "        #     classifer.\n",
        "        #\n",
        "        # Return := Tensor of size (m,) --- This is the logistic loss for each\n",
        "        #     example.\n",
        "\n",
        "        # Fill in the rest\n",
        "        z = torch.matmul(X, w)\n",
        "        loss = torch.log(1 + torch.exp(-y * z))\n",
        "        return loss\n",
        "\n",
        "    @staticmethod\n",
        "    def logistic_gradient(X, y, w):\n",
        "        # Given a batch of samples and labels, compute the batched gradient of\n",
        "        # the logistic loss.\n",
        "        #\n",
        "        # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n",
        "        #     of dimension d\n",
        "        #\n",
        "        # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n",
        "        #\n",
        "        # w := Tensor(float) of size(d,) --- This is the weights of a logistic\n",
        "        #     classifer.\n",
        "        #\n",
        "        # Return := Tensor of size (m,d) --- This is the logistic gradient for each\n",
        "        #     example.\n",
        "        #\n",
        "        # Hint: A very similar gradient was calculated in Homework 0.\n",
        "        # However, that was the sample gradient (with respect to X), whereas\n",
        "        # what we need here is the parameter gradient (with respect to w).\n",
        "\n",
        "        # Fill in the rest\n",
        "\n",
        "        z = X @ w\n",
        "        sigmoid = 1 / (1 + torch.exp(-y * z))\n",
        "        grad = -y.view(-1, 1) * X * (1 - sigmoid).view(-1,1)\n",
        "        return grad\n",
        "\n",
        "    def optimize(self, X, y, niters=100):\n",
        "        # Given a dataset of examples and labels, minimizes the logistic loss\n",
        "        # using standard gradient descent.\n",
        "        #\n",
        "        # This optimizer is written for you, and you only need to implement the\n",
        "        # logistic loss and gradient functions above.\n",
        "        #\n",
        "        # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n",
        "        #     of dimension d\n",
        "        #\n",
        "        # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n",
        "        #\n",
        "        # Return := Tensor of size(d,) --- This is the fitted weights of a\n",
        "        #     logistic regression model\n",
        "\n",
        "        m,d = X.size()\n",
        "        w = torch.zeros(d)\n",
        "        print('Optimizing logistic function...')\n",
        "        for i in range(niters):\n",
        "            loss = self.logistic_loss(X,y,w).mean()\n",
        "            grad = self.logistic_gradient(X,y,w).mean(0)\n",
        "            w -= grad\n",
        "            if i % 50 == 0:\n",
        "                print(i, loss.item())\n",
        "        print('Optimizing done.')\n",
        "        return w\n",
        "\n",
        "def logistic_fit(X, y, optimizer=LogisticOptimizer):\n",
        "    # Given a dataset of examples and labels, fit the weights of the logistic\n",
        "    # regression classifier using the provided loss function and optimizer\n",
        "    #\n",
        "    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n",
        "    #     of dimension d\n",
        "    #\n",
        "    # y := Tensor(int) of size (m,) --- This is a batch of m labels in {-1,1}\n",
        "    #\n",
        "    # Return := Tensor of size (d,) --- This is the fitted weights of the\n",
        "    #     logistic regression model\n",
        "    opt = optimizer()\n",
        "    return opt.optimize(X, y)\n",
        "\n",
        "def logistic_predict(X, w):\n",
        "    # Given a dataset of examples and fitted weights for a logistic regression\n",
        "    # classifier, predict the class\n",
        "    #\n",
        "    # X := Tensor(float) of size(m,d) --- This is a batch of m examples of\n",
        "    #    dimension d\n",
        "    #\n",
        "    # w := Tensor(float) of size (d,) --- This is the fitted weights of the\n",
        "    #    logistic regression model\n",
        "    #\n",
        "    # Return := Tensor of size (m,) --- This is the predicted classes {-1,1}\n",
        "    #    for each example\n",
        "    #\n",
        "    # Hint: Remember that logistic regression expects a label in {-1,1}, and\n",
        "    # not {0,1}\n",
        "\n",
        "    z = torch.matmul(X, w)\n",
        "    predictions = torch.sign(z)\n",
        "    return predictions\n",
        ""
      ],
      "metadata": {
        "id": "8pN0TUK19m7x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your code on the wine dataset!\n",
        "# How does your solution compare to a random linear classifier?\n",
        "# Your solution should get around 75% accuracy on the test set.\n",
        "torch.manual_seed(42)\n",
        "\n",
        "d = X_train.size(1)\n",
        "logistic_weights = {\n",
        "    'zero': torch.zeros(d),\n",
        "    'random': torch.randn(d),\n",
        "    'fitted': logistic_fit(X_train, y_binary_train)\n",
        "}\n",
        "\n",
        "for k,w in logistic_weights.items():\n",
        "    yp_binary_train = logistic_predict(X_train, w)\n",
        "    acc_train = (yp_binary_train == y_binary_train).float().mean()\n",
        "\n",
        "    print(f'Train accuracy [{k}]: {acc_train.item():.2f}')\n",
        "\n",
        "    yp_binary_test = logistic_predict(X_test, w)\n",
        "    acc_test = (yp_binary_test == y_binary_test).float().mean()\n",
        "\n",
        "    print(f'Test accuracy [{k}]: {acc_test.item():.2f}')"
      ],
      "metadata": {
        "id": "cbjOw0Za9pu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba01d6d7-5f6c-424b-bbc8-0d569360b77e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimizing logistic function...\n",
            "0 0.6931473016738892\n",
            "50 0.518214225769043\n",
            "Optimizing done.\n",
            "Train accuracy [zero]: 0.00\n",
            "Test accuracy [zero]: 0.00\n",
            "Train accuracy [random]: 0.54\n",
            "Test accuracy [random]: 0.53\n",
            "Train accuracy [fitted]: 0.75\n",
            "Test accuracy [fitted]: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autograder\n",
        "Be sure you can pass the following four test cases!"
      ],
      "metadata": {
        "id": "OC3HusWT9sdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grader.grade(test_case_id = 'logistic_loss', answer = LogisticOptimizer.logistic_loss)\n",
        "grader.grade(test_case_id = 'logistic_gradient', answer = LogisticOptimizer.logistic_gradient)\n",
        "grader.grade(test_case_id = 'logistic_fit', answer = logistic_fit)\n",
        "grader.grade(test_case_id = 'logistic_predict', answer = logistic_predict)"
      ],
      "metadata": {
        "id": "iBgHs8r-9tS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9a97add-5bef-4762-9274-6a779a32e110"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Linear Regression with Ridge Regression\n",
        "\n",
        "In this second problem, you'll implement a linear regression model. Similarly to the first problem, implement the following functions:\n",
        "\n",
        "1. Loss (3pts) - Given a batch of examples $X$ and labels $y$, compute the batched mean squared error loss for a linear model with weights $w$.\n",
        "2. Fit (4pts) - Given a batch of examples $X$ and labels $y$, find the weights of the optimal linear regression model\n",
        "3. Predict (3pts) - Given the weights $w$ of a linear regression model and new data $X$, predict the most likely label\n",
        "\n",
        "This time, you are not given an optimizer for the fitting function since this problem has an analytic solution. Make sure to test your solution with non-zero ridge regression parameters."
      ],
      "metadata": {
        "id": "FcLy1Sg09vFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regression_loss(X, y, w):\n",
        "    # Given a batch of linear regression outputs and true labels, compute\n",
        "    # the batch of squared error losses. This is *without* the ridge\n",
        "    # regression penalty.\n",
        "    #\n",
        "    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n",
        "    #     of dimension d\n",
        "    #\n",
        "    # y := Tensor(int) of size (m,) --- This is a batch of m real-valued labels\n",
        "    #\n",
        "    # w := Tensor(float) of size(d,) --- This is the weights of a linear\n",
        "    #     classifer\n",
        "    #\n",
        "    # Return := Tensor of size (m,) --- This is the squared loss for each\n",
        "    #     example\n",
        "\n",
        "    # Fill in the rest\n",
        "    z = X @ w\n",
        "    loss = (z - y)**2\n",
        "    return loss\n",
        "\n",
        "def regression_fit(X, y, ridge_penalty=1.0):\n",
        "    # Given a dataset of examples and labels, fit the weights of the linear\n",
        "    # regression classifier using the provided loss function and optimizer\n",
        "    #\n",
        "    # X := Tensor(float) of size (m,d) --- This is a batch of m examples of\n",
        "    #     of dimension d\n",
        "    #\n",
        "    # y := Tensor(float) of size (m,) --- This is a batch of m real-valued\n",
        "    #     labels\n",
        "    #\n",
        "    # ridge_penalty := float --- This is the parameter for ridge regression\n",
        "    #\n",
        "    # Return := Tensor of size (d,) --- This is the fitted weights of the\n",
        "    #     linear regression model\n",
        "    #\n",
        "    # Fill in the rest\n",
        "    w_opt = torch.inverse(torch.t(X) @ X + ridge_penalty * X.size(0) * torch.eye(X.size(1))) @ torch.t(X) @ y\n",
        "\n",
        "    return w_opt\n",
        "\n",
        "def regression_predict(X, w):\n",
        "    # Given a dataset of examples and fitted weights for a linear regression\n",
        "    # classifier, predict the label\n",
        "    #\n",
        "    # X := Tensor(float) of size(m,d) --- This is a batch of m examples of\n",
        "    #    dimension d\n",
        "    #\n",
        "    # w := Tensor(float) of size (d,) --- This is the fitted weights of the\n",
        "    #    linear regression model\n",
        "    #\n",
        "    # Return := Tensor of size (m,) --- This is the predicted real-valued labels\n",
        "    #    for each example\n",
        "    #\n",
        "    # Fill in the rest\n",
        "    return X@w"
      ],
      "metadata": {
        "id": "_kRGPd2g9xFe"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your code on the wine dataset!\n",
        "# How does your solution compare to a random linear classifier?\n",
        "# Your solution should get an average squard error of about 8.6 test set.\n",
        "torch.manual_seed(42)\n",
        "\n",
        "d = X_train.size(1)\n",
        "regression_weights = {\n",
        "    'zero': torch.zeros(d),\n",
        "    'random': torch.randn(d),\n",
        "    'fitted': regression_fit(X_train, y_regression_train)\n",
        "}\n",
        "\n",
        "for k,w in regression_weights.items():\n",
        "    yp_regression_train = regression_predict(X_train, w)\n",
        "    squared_loss_train = regression_loss(X_train, y_regression_train, w).mean()\n",
        "\n",
        "    print(f'Train accuracy [{k}]: {squared_loss_train.item():.2f}')\n",
        "\n",
        "    yp_regression_test = regression_predict(X_test, w)\n",
        "    squared_loss_test = regression_loss(X_test, y_regression_test, w).mean()\n",
        "\n",
        "    print(f'Test accuracy [{k}]: {squared_loss_test.item():.2f}')"
      ],
      "metadata": {
        "id": "AjRqOGqQ90vg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ffe0d1e-f1b0-4f56-a3f4-1b63d22a802f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy [zero]: 32.28\n",
            "Test accuracy [zero]: 32.97\n",
            "Train accuracy [random]: 29.64\n",
            "Test accuracy [random]: 29.55\n",
            "Train accuracy [fitted]: 8.37\n",
            "Test accuracy [fitted]: 8.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autograder\n",
        "Be sure you can pass the following three test cases!"
      ],
      "metadata": {
        "id": "4DqhMV2b92c8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grader.grade(test_case_id = 'regression_loss', answer = regression_loss)\n",
        "grader.grade(test_case_id = 'regression_fit', answer = regression_fit)\n",
        "grader.grade(test_case_id = 'regression_predict', answer = regression_predict)"
      ],
      "metadata": {
        "id": "TyggyMXt92vv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65f83ac-ea23-4198-dc3b-5fe6adda7892"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 4/4 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 3/3 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM and Gradient Descent (10 pts)\n",
        "\n",
        "In this problem, you'll implement (soft margin) support vector machines with gradient descent.\n",
        "+ (2pts) Calculate the objective of the Soft SVM (primal)\n",
        "+ (2pts) Calculate the gradient of the Soft SVM objective\n",
        "+ (4pts) Implement a gradient descent optimizer. Your solution needs to converge to an accurate enough answer.\n",
        "+ (2pts) Make predictions with the Soft SVM\n",
        "\n",
        "Tips:\n",
        "- This assignment is more freeform than previous ones. You're allowed to initialize the parameters of the SVM model however you want, as long as your implemented functions return the right values.\n",
        "- You'll need to play with the values of step size and number of iterations to\n",
        "converge to a good value.\n",
        "- To debug your optimization, print the objective over iterations. Remember that the theory says as long as the learning rate is small enough, for strongly convex problems, we are guaranteed to converge at a certain rate. What does this imply about your solution if it is not converging?\n",
        "- As a sanity check, you can get around 97.5% prediction accuracy and converge to an objective below 0.16.  "
      ],
      "metadata": {
        "id": "f7ETKg-L98OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.optimize._lsq.dogbox import LinearOperator\n",
        "class SoftSVM():\n",
        "    def __init__(self, ndims):\n",
        "        # Here, we initialize the parameters of your soft-SVM model for binary\n",
        "        # classification. Don't change the weight and bias variables as the\n",
        "        # autograder will assume that these exist.\n",
        "        # ndims := integer -- number of dimensions\n",
        "        # no return type\n",
        "\n",
        "        self.weight = torch.zeros(ndims)\n",
        "        self.bias = torch.zeros(1)\n",
        "        self.weight.requires_grad = True\n",
        "        self.bias.requires_grad = True\n",
        "\n",
        "    def objective(self, X, y, l2_reg):\n",
        "        # Calculate the objective of your soft-SVM model\n",
        "        # X := Tensor of size (m,d) -- the input features of m examples with d dimensions\n",
        "        # y := Tensor of size (m) -- the labels for each example in X\n",
        "        # l2_reg := float -- L2 regularization penalty\n",
        "        # Returns a scalar tensor (zero dimensional tensor) -- the loss for the model\n",
        "        z = X @ self.weight.view(-1,1) + self.bias\n",
        "        hinge_loss = torch.mean(torch.clamp(1 - y * z, min=0))\n",
        "        reg_term = l2_reg * (self.weight.norm() ** 2)\n",
        "        loss = hinge_loss + reg_term\n",
        "        return loss\n",
        "\n",
        "    def gradient(self, X, y, l2_reg):\n",
        "        # Calculate the gradient of your soft-SVM model\n",
        "        # X := Tensor of size (m,d) -- the input features of m examples with d dimensions\n",
        "        # y := Tensor of size (m) -- the labels for each example in X\n",
        "        # l2_reg := float -- L2 regularization penalty\n",
        "        # Return Tuple (Tensor, Tensor) -- the tensors corresponds to the weight\n",
        "        # and bias parameters respectively\n",
        "        # Fill in the rest\n",
        "        # First, take the gradient with respect to w,\n",
        "        y = y.view(-1, 1)\n",
        "        z = X @ self.weight.view(-1, 1) + self.bias\n",
        "        hinge_loss_grad = 1 - y * z\n",
        "        margin_violation = hinge_loss_grad > 0\n",
        "\n",
        "        dL_dw = -torch.mean(X * y * margin_violation.float(), axis=0) + 2 * l2_reg * self.weight\n",
        "\n",
        "        dL_db = -torch.mean(y * margin_violation.float())\n",
        "\n",
        "        return (dL_dw, dL_db)\n",
        "\n",
        "\n",
        "    def optimize(self, X, y, l2_reg, learning_rate=0.1, n_iterations=10000):\n",
        "        # Calculate the gradient of your soft-SVM model\n",
        "        # X := Tensor of size (m,d) -- the input features of m examples with d dimensions\n",
        "        # y := Tensor of size (m) -- the labels for each example in X\n",
        "        # l2_reg := float -- L2 regularization penalty\n",
        "\n",
        "        # no return type\n",
        "\n",
        "        # Fill in the rest\n",
        "        for _ in range(n_iterations):\n",
        "          dL_dw, dL_db = self.gradient(X, y, l2_reg)\n",
        "\n",
        "          with torch.no_grad():\n",
        "            self.weight -= learning_rate * dL_dw\n",
        "            self.bias -= learning_rate * dL_db\n",
        "\n",
        "          if _ % 100 == 0:\n",
        "            loss = self.objective(X, y, l2_reg)\n",
        "            print(f'Iteration {_}, Loss: {loss.item()}')\n",
        "\n",
        "            if loss.item() < 0.16:\n",
        "                print(\"Objective is less than 0.16. Stopping.\")\n",
        "                break\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Given an X, make a prediction with the SVM\n",
        "        # X := Tensor of size (m,d) -- features of m examples with d dimensions\n",
        "        # Return a tensor of size (m) -- the prediction labels on the dataset X\n",
        "\n",
        "        # Fill in the rest\n",
        "        z = X @ self.weight + self.bias\n",
        "        preds = torch.sign(z)\n",
        "        return preds.view(-1)"
      ],
      "metadata": {
        "id": "d8oyIkeG-LzH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "#Load dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X,y = torch.from_numpy(cancer['data']), torch.from_numpy(cancer['target'])\n",
        "mu,sigma = X.mean(0,keepdim=True), X.std(0,keepdim=True)\n",
        "X,y = ((X-mu)/sigma).float(),(y - 0.5).sign() # prepare data\n",
        "l2_reg = 0.1\n",
        "print(X.size(), y.size())\n",
        "\n",
        "# Optimize the soft-SVM with gradient descent\n",
        "clf = SoftSVM(X.size(1))\n",
        "clf.optimize(X,y,l2_reg)\n",
        "print(\"\\nSoft SVM objective: \")\n",
        "print(clf.objective(X,y,l2_reg).item())\n",
        "print(\"\\nSoft SVM accuracy: \")\n",
        "(clf.predict(X) == y).float().mean().item()"
      ],
      "metadata": {
        "id": "FntYD7Jy-P9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921a1ae1-c47e-44fa-8107-101be9e58611"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([569, 30]) torch.Size([569])\n",
            "Iteration 0, Loss: 1.0625289678573608\n",
            "Iteration 100, Loss: 1.4414517879486084\n",
            "Iteration 200, Loss: 1.4309495687484741\n",
            "Iteration 300, Loss: 1.42995023727417\n",
            "Iteration 400, Loss: 1.430762529373169\n",
            "Iteration 500, Loss: 1.4305435419082642\n",
            "Iteration 600, Loss: 1.4306000471115112\n",
            "Iteration 700, Loss: 1.4305857419967651\n",
            "Iteration 800, Loss: 1.4315601587295532\n",
            "Iteration 900, Loss: 1.430649757385254\n",
            "Iteration 1000, Loss: 1.4299641847610474\n",
            "Iteration 1100, Loss: 1.4301406145095825\n",
            "Iteration 1200, Loss: 1.4300249814987183\n",
            "Iteration 1300, Loss: 1.4312864542007446\n",
            "Iteration 1400, Loss: 1.4312944412231445\n",
            "Iteration 1500, Loss: 1.4300191402435303\n",
            "Iteration 1600, Loss: 1.4307724237442017\n",
            "Iteration 1700, Loss: 1.4308946132659912\n",
            "Iteration 1800, Loss: 1.429995059967041\n",
            "Iteration 1900, Loss: 1.42985200881958\n",
            "Iteration 2000, Loss: 1.4307090044021606\n",
            "Iteration 2100, Loss: 1.4310346841812134\n",
            "Iteration 2200, Loss: 1.4300028085708618\n",
            "Iteration 2300, Loss: 1.430632472038269\n",
            "Iteration 2400, Loss: 1.4304863214492798\n",
            "Iteration 2500, Loss: 1.4315701723098755\n",
            "Iteration 2600, Loss: 1.430128812789917\n",
            "Iteration 2700, Loss: 1.4301267862319946\n",
            "Iteration 2800, Loss: 1.4303058385849\n",
            "Iteration 2900, Loss: 1.4303076267242432\n",
            "Iteration 3000, Loss: 1.4299832582473755\n",
            "Iteration 3100, Loss: 1.430657148361206\n",
            "Iteration 3200, Loss: 1.430950403213501\n",
            "Iteration 3300, Loss: 1.4306117296218872\n",
            "Iteration 3400, Loss: 1.4300822019577026\n",
            "Iteration 3500, Loss: 1.4302372932434082\n",
            "Iteration 3600, Loss: 1.431308388710022\n",
            "Iteration 3700, Loss: 1.429929256439209\n",
            "Iteration 3800, Loss: 1.4299802780151367\n",
            "Iteration 3900, Loss: 1.4306851625442505\n",
            "Iteration 4000, Loss: 1.4304298162460327\n",
            "Iteration 4100, Loss: 1.4313714504241943\n",
            "Iteration 4200, Loss: 1.429980754852295\n",
            "Iteration 4300, Loss: 1.4309282302856445\n",
            "Iteration 4400, Loss: 1.4311681985855103\n",
            "Iteration 4500, Loss: 1.4312889575958252\n",
            "Iteration 4600, Loss: 1.4304152727127075\n",
            "Iteration 4700, Loss: 1.4303005933761597\n",
            "Iteration 4800, Loss: 1.4302778244018555\n",
            "Iteration 4900, Loss: 1.4311951398849487\n",
            "Iteration 5000, Loss: 1.4308589696884155\n",
            "Iteration 5100, Loss: 1.4312095642089844\n",
            "Iteration 5200, Loss: 1.4304999113082886\n",
            "Iteration 5300, Loss: 1.4312530755996704\n",
            "Iteration 5400, Loss: 1.4302953481674194\n",
            "Iteration 5500, Loss: 1.4304865598678589\n",
            "Iteration 5600, Loss: 1.4301191568374634\n",
            "Iteration 5700, Loss: 1.4316163063049316\n",
            "Iteration 5800, Loss: 1.4304943084716797\n",
            "Iteration 5900, Loss: 1.4304909706115723\n",
            "Iteration 6000, Loss: 1.4303923845291138\n",
            "Iteration 6100, Loss: 1.4299014806747437\n",
            "Iteration 6200, Loss: 1.430973768234253\n",
            "Iteration 6300, Loss: 1.4306585788726807\n",
            "Iteration 6400, Loss: 1.4297969341278076\n",
            "Iteration 6500, Loss: 1.4311679601669312\n",
            "Iteration 6600, Loss: 1.4294923543930054\n",
            "Iteration 6700, Loss: 1.430333137512207\n",
            "Iteration 6800, Loss: 1.4315757751464844\n",
            "Iteration 6900, Loss: 1.4305557012557983\n",
            "Iteration 7000, Loss: 1.4310920238494873\n",
            "Iteration 7100, Loss: 1.4302860498428345\n",
            "Iteration 7200, Loss: 1.4302623271942139\n",
            "Iteration 7300, Loss: 1.4304507970809937\n",
            "Iteration 7400, Loss: 1.4309067726135254\n",
            "Iteration 7500, Loss: 1.4313201904296875\n",
            "Iteration 7600, Loss: 1.4303697347640991\n",
            "Iteration 7700, Loss: 1.4306323528289795\n",
            "Iteration 7800, Loss: 1.4308074712753296\n",
            "Iteration 7900, Loss: 1.4302186965942383\n",
            "Iteration 8000, Loss: 1.429597020149231\n",
            "Iteration 8100, Loss: 1.431470274925232\n",
            "Iteration 8200, Loss: 1.4303828477859497\n",
            "Iteration 8300, Loss: 1.4300837516784668\n",
            "Iteration 8400, Loss: 1.4300178289413452\n",
            "Iteration 8500, Loss: 1.4307208061218262\n",
            "Iteration 8600, Loss: 1.4305850267410278\n",
            "Iteration 8700, Loss: 1.4303292036056519\n",
            "Iteration 8800, Loss: 1.430420160293579\n",
            "Iteration 8900, Loss: 1.430921196937561\n",
            "Iteration 9000, Loss: 1.4312516450881958\n",
            "Iteration 9100, Loss: 1.430188536643982\n",
            "Iteration 9200, Loss: 1.4298381805419922\n",
            "Iteration 9300, Loss: 1.4309643507003784\n",
            "Iteration 9400, Loss: 1.4307578802108765\n",
            "Iteration 9500, Loss: 1.430614709854126\n",
            "Iteration 9600, Loss: 1.4298994541168213\n",
            "Iteration 9700, Loss: 1.4310603141784668\n",
            "Iteration 9800, Loss: 1.4312831163406372\n",
            "Iteration 9900, Loss: 1.4308478832244873\n",
            "\n",
            "Soft SVM objective: \n",
            "1.430503010749817\n",
            "\n",
            "Soft SVM accuracy: \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9753954410552979"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autograder\n",
        "Be sure you can pass the following four test cases!"
      ],
      "metadata": {
        "id": "4TfVUYlF-iKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grader.grade(test_case_id = 'SVM_objective', answer = SoftSVM)\n",
        "grader.grade(test_case_id = 'SVM_gradient', answer = SoftSVM)\n",
        "grader.grade(test_case_id = 'SVM_optimize', answer = SoftSVM)\n",
        "grader.grade(test_case_id = 'SVM_predict', answer = SoftSVM)"
      ],
      "metadata": {
        "id": "fqpFzwc7-hwf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "304ae735-bbd0-4c8c-b042-e5995f8874b4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 4/4 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n",
            "Correct! You earned 2/2 points. You are a star!\n",
            "\n",
            "Your submission has been successfully recorded in the gradebook.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submitting to Gradescope\n",
        "Before submitting to Gradescope, make sure that selecting \"Runtime\" -> \"Restart and run all\" completes all cells without errors.\n",
        "\n",
        "1. Go to the File menu and choose \"Download .ipynb\" and also \"Download .py\". Make sure these files are named homework3.ipynb and homework3.py, respectively\n",
        "2. Go to GradeScope through the canvas page and ensure your class is \"BAN_CIS-5200-001 202330\"\n",
        "3. Select Homework 2\n",
        "4. Upload both files (the .ipynb and the .py)\n",
        "5. PLEASE CHECK THE AUTOGRADER OUTPUT TO ENSURE YOUR SUBMISSION IS PROCESSED CORRECTLY! If this is the case, you should be all set with the programming component of this homework!"
      ],
      "metadata": {
        "id": "9H3yJsDc-8Z0"
      }
    }
  ]
}